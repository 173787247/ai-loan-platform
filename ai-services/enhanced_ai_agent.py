"""
å¢å¼ºç‰ˆAIåŠ©è´·æ‹›æ ‡æ™ºèƒ½ä½“
é›†æˆå¤šç§LLMæ¨¡å‹ï¼Œæä¾›æ›´æ™ºèƒ½çš„å¯¹è¯å’Œå†³ç­–èƒ½åŠ›

@author AI Loan Platform Team
@version 1.1.0
"""

import asyncio
import json
import time
from datetime import datetime
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from enum import Enum
import requests
from loguru import logger

from llm_integration import LLMManager, create_llm_manager
from ai_loan_agent import AILoanAgent, AgentState, UserProfile, LoanPurpose, AgentResponse

class ConversationMode(Enum):
    """å¯¹è¯æ¨¡å¼"""
    PROFESSIONAL = "professional"    # ä¸“ä¸šæ¨¡å¼
    FRIENDLY = "friendly"           # å‹å¥½æ¨¡å¼
    TECHNICAL = "technical"         # æŠ€æœ¯æ¨¡å¼
    SIMPLE = "simple"              # ç®€å•æ¨¡å¼

@dataclass
class AgentCapabilities:
    """æ™ºèƒ½ä½“èƒ½åŠ›"""
    llm_enabled: bool = True
    risk_assessment: bool = True
    smart_matching: bool = True
    document_processing: bool = True
    recommendation: bool = True
    multi_language: bool = True
    voice_support: bool = False
    image_analysis: bool = False

class EnhancedAILoanAgent(AILoanAgent):
    """å¢å¼ºç‰ˆAIåŠ©è´·æ‹›æ ‡æ™ºèƒ½ä½“"""
    
    def __init__(self, base_url: str = "http://localhost:8000", llm_name: str = "gpt-3.5-turbo"):
        super().__init__(base_url)
        self.llm_manager = create_llm_manager()
        self.llm_name = llm_name
        self.conversation_mode = ConversationMode.PROFESSIONAL
        self.capabilities = AgentCapabilities()
        self.conversation_context = []
        self.user_preferences = {}
        
        logger.info(f"å¢å¼ºç‰ˆAIåŠ©è´·æ‹›æ ‡æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨LLM: {llm_name}")
    
    async def start_conversation(self, user_id: int, mode: ConversationMode = ConversationMode.PROFESSIONAL) -> AgentResponse:
        """å¼€å§‹å¯¹è¯ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        self.state = AgentState.PROCESSING
        self.user_profile = None
        self.current_application = None
        self.conversation_mode = mode
        self.conversation_context = []
        
        # ä½¿ç”¨LLMç”Ÿæˆä¸ªæ€§åŒ–æ¬¢è¿æ¶ˆæ¯
        welcome_prompt = self._generate_welcome_prompt(mode)
        
        try:
            welcome_message = await self.llm_manager.generate(
                welcome_prompt,
                llm_name=self.llm_name,
                temperature=0.8,
                max_tokens=500
            )
        except Exception as e:
            logger.warning(f"LLMç”Ÿæˆæ¬¢è¿æ¶ˆæ¯å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¶ˆæ¯: {str(e)}")
            welcome_message = self._get_default_welcome_message()
        
        self.conversation_history.append({
            "role": "assistant",
            "message": welcome_message,
            "timestamp": datetime.now().isoformat(),
            "mode": mode.value
        })
        
        return AgentResponse(
            success=True,
            message=welcome_message,
            data={
                "user_id": user_id,
                "state": self.state.value,
                "mode": mode.value,
                "capabilities": self.capabilities.__dict__
            },
            next_action="collect_user_info",
            confidence=1.0,
            timestamp=datetime.now().isoformat()
        )
    
    async def intelligent_collect_user_info(self, user_data: Dict[str, Any]) -> AgentResponse:
        """æ™ºèƒ½æ”¶é›†ç”¨æˆ·ä¿¡æ¯"""
        try:
            self.state = AgentState.PROCESSING
            
            # ä½¿ç”¨LLMåˆ†æç”¨æˆ·ä¿¡æ¯
            analysis_prompt = self._generate_user_analysis_prompt(user_data)
            
            try:
                analysis = await self.llm_manager.generate(
                    analysis_prompt,
                    llm_name=self.llm_name,
                    temperature=0.7,
                    max_tokens=800
                )
            except Exception as e:
                logger.warning(f"LLMåˆ†æç”¨æˆ·ä¿¡æ¯å¤±è´¥: {str(e)}")
                analysis = self._analyze_user_needs()
            
            # æ„å»ºç”¨æˆ·ç”»åƒ
            self.user_profile = UserProfile(
                user_id=user_data.get("user_id", 0),
                company_name=user_data.get("company_name", ""),
                industry=user_data.get("industry", ""),
                company_size=user_data.get("company_size", ""),
                business_age=user_data.get("business_age", 0),
                annual_revenue=user_data.get("annual_revenue", 0.0),
                monthly_income=user_data.get("monthly_income", 0.0),
                credit_score=user_data.get("credit_score", 0),
                management_experience=user_data.get("management_experience", 0),
                risk_tolerance=user_data.get("risk_tolerance", "medium"),
                preferred_loan_amount=user_data.get("preferred_loan_amount", 0.0),
                preferred_term=user_data.get("preferred_term", 12),
                preferred_rate=user_data.get("preferred_rate", 0.08)
            )
            
            # ç”Ÿæˆä¸ªæ€§åŒ–å›å¤
            response_prompt = self._generate_user_info_response_prompt(user_data, analysis)
            
            try:
                message = await self.llm_manager.generate(
                    response_prompt,
                    llm_name=self.llm_name,
                    temperature=0.8,
                    max_tokens=1000
                )
            except Exception as e:
                logger.warning(f"LLMç”Ÿæˆå›å¤å¤±è´¥: {str(e)}")
                message = self._get_default_user_info_message()
            
            self.conversation_history.append({
                "role": "assistant",
                "message": message,
                "timestamp": datetime.now().isoformat(),
                "analysis": analysis
            })
            
            return AgentResponse(
                success=True,
                message=message,
                data={
                    "user_profile": self.user_profile.__dict__,
                    "analysis": analysis,
                    "state": self.state.value
                },
                next_action="risk_assessment",
                confidence=0.9,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½æ”¶é›†ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {str(e)}")
            return AgentResponse(
                success=False,
                message=f"ä¿¡æ¯æ”¶é›†å¤±è´¥: {str(e)}",
                data={},
                next_action="retry",
                confidence=0.0,
                timestamp=datetime.now().isoformat()
            )
    
    async def intelligent_risk_assessment(self) -> AgentResponse:
        """æ™ºèƒ½é£é™©è¯„ä¼°"""
        try:
            self.state = AgentState.PROCESSING
            
            if not self.user_profile:
                return AgentResponse(
                    success=False,
                    message="è¯·å…ˆæä¾›ç”¨æˆ·ä¿¡æ¯",
                    data={},
                    next_action="collect_user_info",
                    confidence=0.0,
                    timestamp=datetime.now().isoformat()
                )
            
            # è°ƒç”¨åŸæœ‰é£é™©è¯„ä¼°
            risk_response = self.assess_risk()
            
            if not risk_response.success:
                return risk_response
            
            # ä½¿ç”¨LLMå¢å¼ºé£é™©è¯„ä¼°ç»“æœ
            enhancement_prompt = self._generate_risk_enhancement_prompt(risk_response.data)
            
            try:
                enhanced_analysis = await self.llm_manager.generate(
                    enhancement_prompt,
                    llm_name=self.llm_name,
                    temperature=0.7,
                    max_tokens=1200
                )
            except Exception as e:
                logger.warning(f"LLMå¢å¼ºé£é™©è¯„ä¼°å¤±è´¥: {str(e)}")
                enhanced_analysis = risk_response.message
            
            # ç”Ÿæˆä¸ªæ€§åŒ–é£é™©å»ºè®®
            advice_prompt = self._generate_risk_advice_prompt(risk_response.data)
            
            try:
                personalized_advice = await self.llm_manager.generate(
                    advice_prompt,
                    llm_name=self.llm_name,
                    temperature=0.8,
                    max_tokens=800
                )
            except Exception as e:
                logger.warning(f"LLMç”Ÿæˆé£é™©å»ºè®®å¤±è´¥: {str(e)}")
                personalized_advice = "å»ºè®®å’¨è¯¢ä¸“ä¸šé‡‘èé¡¾é—®è·å–è¯¦ç»†å»ºè®®ã€‚"
            
            enhanced_message = f"""
{risk_response.message}

ğŸ¤– AIæ™ºèƒ½åˆ†æï¼š
{enhanced_analysis}

ğŸ’¡ ä¸ªæ€§åŒ–å»ºè®®ï¼š
{personalized_advice}
            """
            
            self.conversation_history.append({
                "role": "assistant",
                "message": enhanced_message,
                "timestamp": datetime.now().isoformat(),
                "enhanced_analysis": enhanced_analysis,
                "personalized_advice": personalized_advice
            })
            
            return AgentResponse(
                success=True,
                message=enhanced_message,
                data={
                    **risk_response.data,
                    "enhanced_analysis": enhanced_analysis,
                    "personalized_advice": personalized_advice
                },
                next_action="smart_matching",
                confidence=0.95,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½é£é™©è¯„ä¼°å¤±è´¥: {str(e)}")
            return AgentResponse(
                success=False,
                message=f"é£é™©è¯„ä¼°å¤±è´¥: {str(e)}",
                data={},
                next_action="retry",
                confidence=0.0,
                timestamp=datetime.now().isoformat()
            )
    
    async def intelligent_smart_matching(self) -> AgentResponse:
        """æ™ºèƒ½åŒ¹é…ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        try:
            self.state = AgentState.PROCESSING
            
            if not self.user_profile:
                return AgentResponse(
                    success=False,
                    message="è¯·å…ˆå®Œæˆé£é™©è¯„ä¼°",
                    data={},
                    next_action="assess_risk",
                    confidence=0.0,
                    timestamp=datetime.now().isoformat()
                )
            
            # è°ƒç”¨åŸæœ‰æ™ºèƒ½åŒ¹é…
            match_response = self.smart_matching()
            
            if not match_response.success:
                return match_response
            
            # ä½¿ç”¨LLMåˆ†æåŒ¹é…ç»“æœ
            analysis_prompt = self._generate_matching_analysis_prompt(match_response.data)
            
            try:
                intelligent_analysis = await self.llm_manager.generate(
                    analysis_prompt,
                    llm_name=self.llm_name,
                    temperature=0.7,
                    max_tokens=1000
                )
            except Exception as e:
                logger.warning(f"LLMåˆ†æåŒ¹é…ç»“æœå¤±è´¥: {str(e)}")
                intelligent_analysis = "åŸºäºæ‚¨çš„éœ€æ±‚ï¼Œæˆ‘ä»¬ä¸ºæ‚¨æ‰¾åˆ°äº†åˆé€‚çš„è´·æ¬¾äº§å“ã€‚"
            
            # ç”Ÿæˆä¸ªæ€§åŒ–æ¨èç†ç”±
            reasoning_prompt = self._generate_recommendation_reasoning_prompt(match_response.data)
            
            try:
                personalized_reasoning = await self.llm_manager.generate(
                    reasoning_prompt,
                    llm_name=self.llm_name,
                    temperature=0.8,
                    max_tokens=800
                )
            except Exception as e:
                logger.warning(f"LLMç”Ÿæˆæ¨èç†ç”±å¤±è´¥: {str(e)}")
                personalized_reasoning = "è¿™äº›äº§å“ç¬¦åˆæ‚¨çš„è´·æ¬¾éœ€æ±‚ã€‚"
            
            enhanced_message = f"""
{match_response.message}

ğŸ§  AIæ™ºèƒ½åˆ†æï¼š
{intelligent_analysis}

ğŸ¯ ä¸ªæ€§åŒ–æ¨èç†ç”±ï¼š
{personalized_reasoning}
            """
            
            self.conversation_history.append({
                "role": "assistant",
                "message": enhanced_message,
                "timestamp": datetime.now().isoformat(),
                "intelligent_analysis": intelligent_analysis,
                "personalized_reasoning": personalized_reasoning
            })
            
            return AgentResponse(
                success=True,
                message=enhanced_message,
                data={
                    **match_response.data,
                    "intelligent_analysis": intelligent_analysis,
                    "personalized_reasoning": personalized_reasoning
                },
                next_action="generate_recommendations",
                confidence=0.9,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½åŒ¹é…å¤±è´¥: {str(e)}")
            return AgentResponse(
                success=False,
                message=f"æ™ºèƒ½åŒ¹é…å¤±è´¥: {str(e)}",
                data={},
                next_action="retry",
                confidence=0.0,
                timestamp=datetime.now().isoformat()
            )
    
    async def chat_with_llm(self, user_message: str) -> AgentResponse:
        """ä¸LLMå¯¹è¯"""
        try:
            self.state = AgentState.PROCESSING
            
            # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
            messages = [
                {
                    "role": "system",
                    "content": self._get_system_prompt()
                }
            ]
            
            # æ·»åŠ å¯¹è¯å†å²
            for msg in self.conversation_history[-5:]:  # åªä¿ç•™æœ€è¿‘5æ¡
                messages.append({
                    "role": msg["role"],
                    "content": msg["message"]
                })
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            messages.append({
                "role": "user",
                "content": user_message
            })
            
            # è°ƒç”¨LLM
            response = await self.llm_manager.chat(
                messages,
                llm_name=self.llm_name,
                temperature=0.8,
                max_tokens=1000
            )
            
            self.conversation_history.append({
                "role": "assistant",
                "message": response,
                "timestamp": datetime.now().isoformat(),
                "llm_generated": True
            })
            
            return AgentResponse(
                success=True,
                message=response,
                data={"llm_generated": True},
                next_action="continue",
                confidence=0.9,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            logger.error(f"LLMå¯¹è¯å¤±è´¥: {str(e)}")
            return AgentResponse(
                success=False,
                message=f"å¯¹è¯å¤±è´¥: {str(e)}",
                data={},
                next_action="retry",
                confidence=0.0,
                timestamp=datetime.now().isoformat()
            )
    
    def switch_llm(self, llm_name: str) -> bool:
        """åˆ‡æ¢LLMæ¨¡å‹"""
        try:
            if llm_name in self.llm_manager.list_llms():
                self.llm_name = llm_name
                logger.info(f"LLMæ¨¡å‹å·²åˆ‡æ¢åˆ°: {llm_name}")
                return True
            else:
                logger.error(f"LLMæ¨¡å‹ {llm_name} ä¸å¯ç”¨")
                return False
        except Exception as e:
            logger.error(f"åˆ‡æ¢LLMæ¨¡å‹å¤±è´¥: {str(e)}")
            return False
    
    def set_conversation_mode(self, mode: ConversationMode):
        """è®¾ç½®å¯¹è¯æ¨¡å¼"""
        self.conversation_mode = mode
        logger.info(f"å¯¹è¯æ¨¡å¼å·²è®¾ç½®ä¸º: {mode.value}")
    
    def get_available_llms(self) -> List[str]:
        """è·å–å¯ç”¨çš„LLMæ¨¡å‹"""
        return self.llm_manager.list_llms()
    
    # ç§æœ‰æ–¹æ³•
    def _generate_welcome_prompt(self, mode: ConversationMode) -> str:
        """ç”Ÿæˆæ¬¢è¿æ¶ˆæ¯æç¤º"""
        mode_descriptions = {
            ConversationMode.PROFESSIONAL: "ä¸“ä¸šã€æ­£å¼ã€æƒå¨",
            ConversationMode.FRIENDLY: "å‹å¥½ã€äº²åˆ‡ã€æ¸©æš–",
            ConversationMode.TECHNICAL: "æŠ€æœ¯ã€è¯¦ç»†ã€ç²¾ç¡®",
            ConversationMode.SIMPLE: "ç®€å•ã€æ˜“æ‡‚ã€ç›´æ¥"
        }
        
        return f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©è´·æ‹›æ ‡æ™ºèƒ½ä½“ï¼Œè¯·ç”¨{mode_descriptions[mode]}çš„è¯­æ°”ç”Ÿæˆä¸€æ®µæ¬¢è¿æ¶ˆæ¯ã€‚

è¦æ±‚ï¼š
1. ä»‹ç»ä½ çš„èº«ä»½å’Œèƒ½åŠ›
2. è¯´æ˜ä½ èƒ½æä¾›çš„æœåŠ¡
3. å¼•å¯¼ç”¨æˆ·å¼€å§‹è´·æ¬¾ç”³è¯·æµç¨‹
4. ä½“ç°ä¸“ä¸šæ€§å’Œå¯ä¿¡åº¦
5. è¯­è¨€è¦è‡ªç„¶æµç•…ï¼Œç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯

è¯·ç”Ÿæˆä¸€æ®µ200-300å­—çš„æ¬¢è¿æ¶ˆæ¯ã€‚
        """
    
    def _generate_user_analysis_prompt(self, user_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆç”¨æˆ·åˆ†ææç¤º"""
        return f"""
è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·ä¿¡æ¯ï¼Œå¹¶æä¾›ä¸“ä¸šçš„è´·æ¬¾å»ºè®®ï¼š

ç”¨æˆ·ä¿¡æ¯ï¼š
- ä¼ä¸šåç§°ï¼š{user_data.get('company_name', 'æœªçŸ¥')}
- æ‰€å±è¡Œä¸šï¼š{user_data.get('industry', 'æœªçŸ¥')}
- ä¼ä¸šè§„æ¨¡ï¼š{user_data.get('company_size', 'æœªçŸ¥')}
- ç»è¥å¹´é™ï¼š{user_data.get('business_age', 0)}å¹´
- å¹´è¥ä¸šæ”¶å…¥ï¼š{user_data.get('annual_revenue', 0):,.0f}å…ƒ
- æœˆæ”¶å…¥ï¼š{user_data.get('monthly_income', 0):,.0f}å…ƒ
- ä¿¡ç”¨è¯„åˆ†ï¼š{user_data.get('credit_score', 0)}
- ç®¡ç†ç»éªŒï¼š{user_data.get('management_experience', 0)}å¹´
- é£é™©åå¥½ï¼š{user_data.get('risk_tolerance', 'æœªçŸ¥')}
- æœŸæœ›è´·æ¬¾é‡‘é¢ï¼š{user_data.get('preferred_loan_amount', 0):,.0f}å…ƒ
- æœŸæœ›è´·æ¬¾æœŸé™ï¼š{user_data.get('preferred_term', 0)}ä¸ªæœˆ
- æœŸæœ›åˆ©ç‡ï¼š{user_data.get('preferred_rate', 0):.2%}

è¯·ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œåˆ†æï¼š
1. ä¼ä¸šåŸºæœ¬æƒ…å†µè¯„ä¼°
2. è´·æ¬¾éœ€æ±‚åˆç†æ€§åˆ†æ
3. é£é™©å› ç´ è¯†åˆ«
4. æ¨èè´·æ¬¾ç±»å‹å’Œæ¡ä»¶
5. ç”³è¯·å»ºè®®å’Œæ³¨æ„äº‹é¡¹

è¯·ç”¨ä¸“ä¸šã€å®¢è§‚çš„è¯­è¨€è¿›è¡Œåˆ†æï¼Œå¹¶æä¾›å…·ä½“çš„å»ºè®®ã€‚
        """
    
    def _generate_user_info_response_prompt(self, user_data: Dict[str, Any], analysis: str) -> str:
        """ç”Ÿæˆç”¨æˆ·ä¿¡æ¯å›å¤æç¤º"""
        return f"""
åŸºäºä»¥ä¸‹ç”¨æˆ·ä¿¡æ¯å’ŒAIåˆ†æï¼Œç”Ÿæˆä¸€æ®µä¸ªæ€§åŒ–çš„å›å¤ï¼š

ç”¨æˆ·ä¿¡æ¯ï¼š{json.dumps(user_data, ensure_ascii=False, indent=2)}

AIåˆ†æï¼š{analysis}

è¯·ç”Ÿæˆä¸€æ®µå›å¤ï¼Œè¦æ±‚ï¼š
1. ç¡®è®¤ç”¨æˆ·ä¿¡æ¯æ”¶é›†å®Œæˆ
2. ç®€è¦æ€»ç»“ç”¨æˆ·çš„ä¼ä¸šæƒ…å†µ
3. åŸºäºåˆ†æç»“æœæä¾›åˆæ­¥å»ºè®®
4. å¼•å¯¼ç”¨æˆ·è¿›å…¥ä¸‹ä¸€æ­¥ï¼ˆé£é™©è¯„ä¼°ï¼‰
5. è¯­è¨€è¦ä¸“ä¸šä¸”å‹å¥½
6. é•¿åº¦æ§åˆ¶åœ¨300-400å­—

è¯·ç”Ÿæˆå›å¤å†…å®¹ã€‚
        """
    
    def _generate_risk_enhancement_prompt(self, risk_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆé£é™©å¢å¼ºåˆ†ææç¤º"""
        return f"""
åŸºäºä»¥ä¸‹é£é™©è¯„ä¼°æ•°æ®ï¼Œæä¾›æ›´æ·±å…¥çš„é£é™©åˆ†æï¼š

é£é™©æ•°æ®ï¼š{json.dumps(risk_data, ensure_ascii=False, indent=2)}

è¯·ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œæ·±å…¥åˆ†æï¼š
1. é£é™©ç­‰çº§çš„å…·ä½“å«ä¹‰å’Œå½±å“
2. å„é¡¹é£é™©è¯„åˆ†çš„è¯¦ç»†è§£é‡Š
3. è¡Œä¸šå’Œå¸‚åœºå› ç´ å¯¹é£é™©çš„å½±å“
4. å†å²æ•°æ®å’Œè¶‹åŠ¿åˆ†æ
5. é£é™©ç¼“è§£çš„å…·ä½“å»ºè®®
6. å¯¹è´·æ¬¾æ¡ä»¶çš„å½±å“åˆ†æ

è¯·ç”¨ä¸“ä¸šã€å®¢è§‚çš„è¯­è¨€è¿›è¡Œåˆ†æï¼Œå¹¶æä¾›å¯æ“ä½œçš„å»ºè®®ã€‚
        """
    
    def _generate_risk_advice_prompt(self, risk_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆé£é™©å»ºè®®æç¤º"""
        return f"""
åŸºäºä»¥ä¸‹é£é™©è¯„ä¼°ç»“æœï¼Œä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–çš„é£é™©å»ºè®®ï¼š

é£é™©æ•°æ®ï¼š{json.dumps(risk_data, ensure_ascii=False, indent=2)}

è¯·æä¾›ä»¥ä¸‹å»ºè®®ï¼š
1. é’ˆå¯¹å½“å‰é£é™©ç­‰çº§çš„å…·ä½“å»ºè®®
2. å¦‚ä½•é™ä½é£é™©çš„å…·ä½“æªæ–½
3. ç”³è¯·è´·æ¬¾æ—¶çš„æ³¨æ„äº‹é¡¹
4. å¯èƒ½éœ€è¦æä¾›çš„é¢å¤–ææ–™
5. å»ºè®®çš„è´·æ¬¾æ¡ä»¶è°ƒæ•´
6. é£é™©ç›‘æ§å’Œç®¡ç†çš„å»ºè®®

è¯·ç”¨å®ç”¨ã€å…·ä½“çš„è¯­è¨€æä¾›å»ºè®®ï¼Œè®©ç”¨æˆ·èƒ½å¤Ÿç†è§£å’Œæ‰§è¡Œã€‚
        """
    
    def _generate_matching_analysis_prompt(self, match_data: Dict[str, Any]) -> str:
        """ç”ŸæˆåŒ¹é…åˆ†ææç¤º"""
        return f"""
åŸºäºä»¥ä¸‹æ™ºèƒ½åŒ¹é…ç»“æœï¼Œæä¾›æ·±å…¥çš„äº§å“åˆ†æï¼š

åŒ¹é…æ•°æ®ï¼š{json.dumps(match_data, ensure_ascii=False, indent=2)}

è¯·ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œåˆ†æï¼š
1. åŒ¹é…ç®—æ³•çš„é€»è¾‘å’Œä¾æ®
2. å„æ¨èäº§å“çš„ä¼˜åŠ£åŠ¿åˆ†æ
3. äº§å“ä¸ç”¨æˆ·éœ€æ±‚çš„åŒ¹é…åº¦åˆ†æ
4. åˆ©ç‡ã€æœŸé™ã€æ¡ä»¶çš„å…·ä½“åˆ†æ
5. ç”³è¯·éš¾åº¦å’ŒæˆåŠŸç‡è¯„ä¼°
6. äº§å“ç»„åˆå»ºè®®

è¯·ç”¨ä¸“ä¸šã€å®¢è§‚çš„è¯­è¨€è¿›è¡Œåˆ†æï¼Œå¸®åŠ©ç”¨æˆ·åšå‡ºæ˜æ™ºçš„é€‰æ‹©ã€‚
        """
    
    def _generate_recommendation_reasoning_prompt(self, match_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ¨èç†ç”±æç¤º"""
        return f"""
åŸºäºä»¥ä¸‹åŒ¹é…ç»“æœï¼Œä¸ºç”¨æˆ·ç”Ÿæˆä¸ªæ€§åŒ–çš„æ¨èç†ç”±ï¼š

åŒ¹é…æ•°æ®ï¼š{json.dumps(match_data, ensure_ascii=False, indent=2)}

è¯·ä¸ºæ¯ä¸ªæ¨èäº§å“ç”Ÿæˆæ¨èç†ç”±ï¼š
1. ä¸ºä»€ä¹ˆè¿™ä¸ªäº§å“é€‚åˆç”¨æˆ·
2. äº§å“çš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯ä»€ä¹ˆ
3. ä¸ç”¨æˆ·éœ€æ±‚çš„åŒ¹é…ç‚¹
4. ç”³è¯·æˆåŠŸçš„å¯èƒ½æ€§
5. éœ€è¦æ³¨æ„çš„é£é™©ç‚¹
6. ç”³è¯·å»ºè®®å’ŒæŠ€å·§

è¯·ç”¨ç®€æ´ã€æœ‰è¯´æœåŠ›çš„è¯­è¨€ç”Ÿæˆæ¨èç†ç”±ã€‚
        """
    
    def _get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤º"""
        return """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©è´·æ‹›æ ‡æ™ºèƒ½ä½“ï¼Œä¸“é—¨ä¸ºå°å¾®ä¼ä¸šæä¾›è´·æ¬¾å’¨è¯¢æœåŠ¡ã€‚

ä½ çš„èƒ½åŠ›åŒ…æ‹¬ï¼š
1. é£é™©è¯„ä¼°å’Œåˆ†æ
2. æ™ºèƒ½äº§å“åŒ¹é…
3. ä¸ªæ€§åŒ–æ¨è
4. è´·æ¬¾æµç¨‹æŒ‡å¯¼
5. é‡‘èçŸ¥è¯†è§£ç­”

ä½ çš„ç‰¹ç‚¹ï¼š
- ä¸“ä¸šã€å‡†ç¡®ã€å¯é 
- è¯­è¨€ç®€æ´æ˜äº†
- æä¾›å®ç”¨å»ºè®®
- ä¿æŠ¤ç”¨æˆ·éšç§
- éµå®ˆé‡‘èæ³•è§„

è¯·å§‹ç»ˆä»¥ä¸“ä¸šã€å‹å¥½çš„æ€åº¦ä¸ºç”¨æˆ·æä¾›æœåŠ¡ã€‚
        """
    
    def _get_default_welcome_message(self) -> str:
        """è·å–é»˜è®¤æ¬¢è¿æ¶ˆæ¯"""
        return """
ğŸ¤– æ¬¢è¿ä½¿ç”¨AIæ™ºèƒ½åŠ©è´·æ‹›æ ‡å¹³å°ï¼

æˆ‘æ˜¯æ‚¨çš„ä¸“å±AIåŠ©è´·é¡¾é—®ï¼Œæ‹¥æœ‰å…ˆè¿›çš„äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œå¯ä»¥ä¸ºæ‚¨æä¾›ï¼š

â€¢ ğŸ“Š æ™ºèƒ½é£é™©è¯„ä¼° - åŸºäºå¤§æ•°æ®å’ŒAIç®—æ³•
â€¢ ğŸ¯ ç²¾å‡†äº§å“åŒ¹é… - ä»æµ·é‡äº§å“ä¸­æ‰¾åˆ°æœ€é€‚åˆçš„
â€¢ ğŸ’¡ ä¸ªæ€§åŒ–æ¨è - æ ¹æ®æ‚¨çš„å…·ä½“æƒ…å†µå®šåˆ¶æ–¹æ¡ˆ
â€¢ ğŸ“‹ å…¨ç¨‹æµç¨‹æŒ‡å¯¼ - ä»ç”³è¯·åˆ°æ”¾æ¬¾çš„å®Œæ•´æœåŠ¡
â€¢ ğŸ” ä¸“ä¸šé‡‘èå’¨è¯¢ - è§£ç­”æ‚¨çš„å„ç§ç–‘é—®

è¯·å‘Šè¯‰æˆ‘æ‚¨çš„åŸºæœ¬ä¿¡æ¯ï¼Œæˆ‘å°†ä¸ºæ‚¨æä¾›æœ€ä¸“ä¸šçš„è´·æ¬¾è§£å†³æ–¹æ¡ˆï¼
        """
    
    def _get_default_user_info_message(self) -> str:
        """è·å–é»˜è®¤ç”¨æˆ·ä¿¡æ¯æ¶ˆæ¯"""
        return """
âœ… ç”¨æˆ·ä¿¡æ¯æ”¶é›†å®Œæˆï¼

æ„Ÿè°¢æ‚¨æä¾›è¯¦ç»†çš„ä¼ä¸šä¿¡æ¯ã€‚åŸºäºæ‚¨çš„æƒ…å†µï¼Œæˆ‘å·²ç»åˆæ­¥åˆ†æäº†æ‚¨çš„è´·æ¬¾éœ€æ±‚ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘å°†ä¸ºæ‚¨è¿›è¡Œä¸“ä¸šçš„é£é™©è¯„ä¼°ï¼Œè¿™å°†å¸®åŠ©æˆ‘ä»¬æ‰¾åˆ°æœ€é€‚åˆæ‚¨çš„è´·æ¬¾äº§å“ã€‚

è¯·ç¨ç­‰ï¼Œé£é™©è¯„ä¼°å³å°†å¼€å§‹...
        """

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ä½¿ç”¨ç¤ºä¾‹"""
    # åˆ›å»ºå¢å¼ºç‰ˆæ™ºèƒ½ä½“
    agent = EnhancedAILoanAgent()
    
    # åˆ—å‡ºå¯ç”¨çš„LLMæ¨¡å‹
    print("å¯ç”¨çš„LLMæ¨¡å‹:")
    for llm in agent.get_available_llms():
        print(f"- {llm}")
    
    # å¼€å§‹å¯¹è¯
    response = await agent.start_conversation(user_id=1, mode=ConversationMode.PROFESSIONAL)
    print(f"æ¬¢è¿æ¶ˆæ¯: {response.message}")
    
    # æ¨¡æ‹Ÿç”¨æˆ·ä¿¡æ¯
    user_data = {
        "user_id": 1,
        "company_name": "æµ‹è¯•ç§‘æŠ€æœ‰é™å…¬å¸",
        "industry": "åˆ¶é€ ä¸š",
        "company_size": "small",
        "business_age": 3,
        "annual_revenue": 2000000,
        "monthly_income": 200000,
        "credit_score": 720,
        "management_experience": 5,
        "risk_tolerance": "medium",
        "preferred_loan_amount": 500000,
        "preferred_term": 24,
        "preferred_rate": 0.08
    }
    
    # æ™ºèƒ½æ”¶é›†ç”¨æˆ·ä¿¡æ¯
    response = await agent.intelligent_collect_user_info(user_data)
    print(f"ç”¨æˆ·ä¿¡æ¯åˆ†æ: {response.message}")
    
    # æ™ºèƒ½é£é™©è¯„ä¼°
    response = await agent.intelligent_risk_assessment()
    print(f"é£é™©è¯„ä¼°: {response.message}")
    
    # æ™ºèƒ½åŒ¹é…
    response = await agent.intelligent_smart_matching()
    print(f"æ™ºèƒ½åŒ¹é…: {response.message}")

if __name__ == "__main__":
    asyncio.run(main())
